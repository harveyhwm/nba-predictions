{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fcd2fa7-b3e5-44c0-86cf-61aa5fc7a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium-trigger-1 must be invoked from outside the lambda flow.  The others are all in a connected chain...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4eaf2-71b7-4723-8bc0-2a63133d88f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **SELENIUM MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "427fe889-d059-4092-bee7-cb46f163ed67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# body of lambda function - works locally as well as on lambda, with platform-agnostic logic\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from selenium import webdriver\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import platform\n",
    "import zipfile\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "mypath = os.path.join('/'.join(os.getcwd().split('/')[:-1]), 'data')\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def make_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--single-process')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    if platform.system() == 'Linux':\n",
    "        options.binary_location = '/opt/headless-chromium'\n",
    "        chromedriver_path = '/opt/chromedriver'\n",
    "    else:\n",
    "        chromedriver_path = '/Users/harveymanhood/chromedriver'\n",
    "    return webdriver.Chrome(chromedriver_path, options=options)\n",
    "\n",
    "\n",
    "def write_s3(file_path, myfile, bucket='hwm-nba', dedupe_cols=None, sort=None, ascending=True, compression='zip'):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    if type(myfile) == pd.core.frame.DataFrame:\n",
    "        if sort is not None:\n",
    "            myfile = myfile.sort_values(by=sort, ascending=ascending)\n",
    "        if dedupe_cols is not None:\n",
    "            myfile = myfile.drop_duplicates(subset=dedupe_cols, keep='first')\n",
    "        output_buffer = BytesIO() if compression == 'zip' else StringIO()\n",
    "        if compression == 'zip': file_path = '.'.join(file_path.split('.')[:-1])+'.zip'\n",
    "        myfile.to_csv(output_buffer, index=False, compression={'method':compression, 'archive_name':file_name})\n",
    "        myfile = output_buffer\n",
    "    s3_resource.Object(bucket, file_path).put(Body=myfile.getvalue())    \n",
    "\n",
    "\n",
    "def read_s3(file_path, bucket='hwm-nba', output=None, columns=None):\n",
    "    data = s3_resource.Object(bucket, file_path).get()['Body'].read()\n",
    "    if file_path[-3:] == 'zip':\n",
    "        data = zipfile.ZipFile(BytesIO(data))\n",
    "        data = data.read(data.namelist()[0])\n",
    "    data = data.decode('utf-8')\n",
    "    if output == 'dataframe':\n",
    "        kwargs = {'header': 0}\n",
    "        if columns is not None: kwargs['names'] = columns\n",
    "        data = pd.read_csv(StringIO(data), **kwargs)\n",
    "    return data\n",
    "\n",
    "\n",
    "def append_s3(file_path, mydata, bucket='hwm-nba', output=None, columns=None, dedupe_cols=None, sort=None, ascending=True):\n",
    "    data = read_s3(file_path, bucket=bucket, output=output, columns=columns)\n",
    "    mydata = pd.concat([data, mydata], axis=0)\n",
    "    file_path = file_path = '.'.join(file_path.split('.')[:-1])+'.csv'\n",
    "    write_s3(file_path, mydata.reset_index(drop=True), bucket=bucket, dedupe_cols=dedupe_cols, sort=sort, ascending=ascending)\n",
    "\n",
    "\n",
    "# temp function for parsing\n",
    "def append_local(file_path, mydata, output=None, columns=None, drop_duplicates=True, sort=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "    mydata = pd.concat([mydata, data], axis=0).drop_duplicates(keep='last').sort_values(by=['Date','Detail Path'], ascending=True)\n",
    "    mydata.to_csv(file_path, index=False)\n",
    "    zip_write(file_path)\n",
    "\n",
    "\n",
    "def record_status(data):\n",
    "    detail_unique = np.unique(data['Detail Path'])\n",
    "    ones = np.ones(len(detail_unique),dtype='int')\n",
    "    detail_unique = pd.DataFrame(np.transpose([detail_unique, ones]), columns=['Detail Path', 'Count'])\n",
    "    games = read_s3('data/games.zip', output='dataframe')\n",
    "    games = games.merge(detail_unique, how='left', on=['Detail Path'])\n",
    "    games['Detail Data'] = games.apply(lambda r: 1 if r['Count']==1 else r['Detail Data'], axis=1)\n",
    "    del games['Count']\n",
    "    if platform.system() != 'Linux':\n",
    "        games.to_csv(os.path.join(mypath, 'games.csv'), index=False)\n",
    "        games.to_csv(os.path.join(mypath, 'games.zip'), index=False, compression = dict(method='zip', archive_name='games.csv'))\n",
    "    write_s3('data/games.csv', games, dedupe_cols=list(games.columns)[:-2], sort=['Date','Detail Path'])\n",
    "\n",
    "\n",
    "def retrieve_dates(dates, table='games', hour_offset=-100): # -100 pulls 4-5 days into the future for prediction purposes        \n",
    "    days = int(hour_offset//24)\n",
    "    hours = hour_offset%24\n",
    "    max_date = datetime.datetime.today() - datetime.timedelta(days=days, hours=hours)    \n",
    "    if type(dates) == list:\n",
    "        dates.sort()\n",
    "        d1, d2 = datetime.datetime.strptime(dates[0], '%Y-%m-%d'), datetime.datetime.strptime(dates[1], '%Y-%m-%d')\n",
    "        dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "    elif dates == 'infer':\n",
    "        games = read_s3('data/games.zip', output='dataframe')\n",
    "        if table != 'games':\n",
    "            games = games[(games['Detail Data'] == 0) & (games['AS'] != '-')]\n",
    "            dates = list(np.unique(games['Date']))\n",
    "        else:                                          \n",
    "            existing = list(np.unique(games['Date']))\n",
    "            max_played = np.max(np.unique(games[games['AS'] != '-']['Date']))\n",
    "            min_date = min(existing)\n",
    "            d1, d2 = datetime.datetime.strptime(min_date, '%Y-%m-%d'), max_date\n",
    "            dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "            dates = [d for d in dates if (d not in existing or d > max_played)]    \n",
    "    else:          \n",
    "        dates = [datetime.datetime.strftime(max_date - datetime.timedelta(days=d), '%Y-%m-%d') for d in range(dates)]\n",
    "    return dates\n",
    "\n",
    "\n",
    "def retrieve_urls(df=False, **params):\n",
    "    site_root = 'https://www.nba.com'\n",
    "    date_param = 'dates' if 'dates' in params.keys() else 'num_pages'\n",
    "    dates = retrieve_dates(params[date_param], table=params['table'])\n",
    "    if params['table'] == 'games':\n",
    "        game_ids = [None for d in dates] # to give symmetry\n",
    "        urls = [site_root+'/games?date='+d for d in dates]\n",
    "    else:\n",
    "        games = read_s3('data/games.zip', output='dataframe')\n",
    "        if 'rand' in params.keys():\n",
    "            games = games[games['Rand'].isin(params['rand'])]\n",
    "            games[games['Rand'].isin(params['rand'])]\n",
    "        games['AS'] = games['AS'].apply(lambda x: int(x) if str(x).split('.')[0].isdigit() else 0)\n",
    "        games['HS'] = games['HS'].apply(lambda x: int(x) if str(x).split('.')[0].isdigit() else 0)\n",
    "        new_games = games[(games['Date'].isin(dates)) & (games['Away'] != 'No Games') & (games['AS'] != 0)]\n",
    "        if params['allow_repeats'] == 'false': new_games = new_games[new_games['Detail Data']==0]\n",
    "        dates, game_ids = np.array(new_games['Date']), np.array(new_games['Detail Path'])\n",
    "        detail_url = '/play-by-play?period=All' # if params['table'] == 'plays' else 'box-score'\n",
    "        urls = [site_root+'/game/'+g+detail_url for g in game_ids]\n",
    "    if df is True:\n",
    "        df = pd.DataFrame([urls, game_ids, dates]).T\n",
    "        df.columns = ['urls', 'game_ids', 'dates']\n",
    "        return df\n",
    "    return urls, game_ids, dates\n",
    "\n",
    "\n",
    "def scrape(url, date, driver, **params):\n",
    "    driver.get(url)\n",
    "    delay = 8\n",
    "    try:\n",
    "        myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'script[id=\"__NEXT_DATA__\"]')))\n",
    "    except TimeoutException:\n",
    "        return 'incomplete'\n",
    "    page_source = driver.page_source\n",
    "    if params['upload_scrape'] == 'true':\n",
    "        file_path = 'scrapes/'+params['table']+'_'+date+'.txt'\n",
    "        write_s3(file_path, StringIO(page_source))\n",
    "    return page_source\n",
    "\n",
    "\n",
    "def zip_write(file, dest=None):\n",
    "    if dest is None:\n",
    "        dest = file.split('.')[0]+'.zip'\n",
    "    my_zip = zipfile.ZipFile(dest, 'w', zipfile.ZIP_DEFLATED)\n",
    "    my_zip.write(file)\n",
    "    my_zip.close()\n",
    "\n",
    "\n",
    "def fetch_nba_teams():\n",
    "    return ['ATL', 'BKN', 'BOS', 'CHA', 'CHH', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GOS', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM',\n",
    "            'MIA', 'MIL', 'MIN', 'NJN', 'NOH', 'NOK', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI', 'PHL', 'PHX', 'POR', 'SAC', 'SAN', 'SAS',\n",
    "            'SEA', 'TOR', 'UTA', 'UTH', 'VAN', 'WAS']\n",
    "\n",
    "\n",
    "def parse(page_source, game_id, date, **params):\n",
    "    teams = fetch_nba_teams()\n",
    "    soup = BS(page_source, 'html.parser')\n",
    "    if params['table'] == 'games':\n",
    "        try:\n",
    "            columns = ['Date', 'Away', 'AS', 'Home', 'HS', 'OT', 'Detail Path', 'Game Type', 'Detail Data', 'Rand']\n",
    "            data = soup.select('div[class*=\"gamecard\"]')\n",
    "            if data == []:\n",
    "                game_data = [[date, 'No Games', '-', 'No Games', '-', '-', '-', '-', 2, np.random.randint(10)]]\n",
    "            for i, d in enumerate(data):\n",
    "                box_score = d.select('a[data-id*=\"box-score\"], a[data-id*=\"preview\"]')[0]\n",
    "                if len(box_score) == 0:\n",
    "                    continue\n",
    "                game_url = box_score.get_attribute_list('href')[0]\n",
    "                game = re.split('\\\\/|\\\\-', game_url.upper())\n",
    "                game_id = game_url.split('game/')[1].split('/box-score')[0]\n",
    "                gametype = 'playoffs' if (len(d.select('[class*=\"gameSeriesText\"]')) > 0 and game[2] in teams and game[4] in teams) \\\n",
    "                    else 'preseason' if d.select('[data-is-preseason]')[0].get_attribute_list('data-is-preseason')[0] == 'true' else 'regular'\n",
    "                scores = [c.get_text() for c in d.select('p[class*=\"MatchupCardScore\"]')]\n",
    "                scores = ['-', '-'] if len(scores) < 2 else scores\n",
    "                overtime_flag = d.select('p[class*=\"GameCardMatchupStatusText\"]')[0].get_text().upper().split('/')[-1][-1].replace('T','1')\n",
    "                overtime = 'N' if (overtime_flag == 'L') or (scores[-1] == '-') else 'Y'\n",
    "                if (overtime == 'Y') and (scores[-1] != '-'): scores[-1] = overtime_flag + scores[-1]\n",
    "                game_data_item = [date, game[2], scores[0], game[4], scores[-1], overtime, game_id, gametype, 0, np.random.randint(10)]\n",
    "                if i==0:\n",
    "                    game_data = [game_data_item]\n",
    "                else:\n",
    "                    game_data.append(game_data_item)\n",
    "            print(date, end=' ')\n",
    "            return {'games': pd.DataFrame(game_data, columns=columns)}\n",
    "        except:\n",
    "            return {}\n",
    "    elif params['table'] == 'game_details':\n",
    "        box_cols = ['Date', 'Detail Path', 'Player ID', 'First Name', 'Last Name', 'Name', 'Slug', 'Position',\n",
    "                   'Comment', 'Jersey', 'Home', 'MIN', 'FGM', 'FGA', 'FG%', '3PM', '3PA', '3P%', 'FTM', 'FTA',\n",
    "                    'FT%', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PTS', '+/-', ]\n",
    "        play_cols = ['Date', 'Detail Path', 'Action Number', 'Clock', 'Period', 'Team ID', 'Team Tricode', 'Person Id', 'Player Name',\n",
    "                     'Player Init', 'xLegacy', 'yLegacy', 'Shot Distance', 'Shot Result', 'Field Goal', 'Score Home', 'Score Away',\n",
    "                     'Points Total', 'Location', 'Description', 'Action Type', 'Sub-Type', 'Video Available', 'Action Id']\n",
    "        data = soup.select('script[id=\"__NEXT_DATA__\"]')\n",
    "        try:\n",
    "            home_stats = pd.DataFrame.from_dict(json.loads(data[0].text)['props']['pageProps']['game']['homeTeam']['players'])\n",
    "            home_stats['Home'] = 'Y'\n",
    "            away_stats = pd.DataFrame.from_dict(json.loads(data[0].text)['props']['pageProps']['game']['awayTeam']['players'])\n",
    "            away_stats['Home'] = 'N'\n",
    "            box_data = pd.concat([home_stats, away_stats], axis=0).reset_index(drop=True)\n",
    "            try:\n",
    "                for key in box_data['statistics'][0].keys():\n",
    "                    box_data[key] = box_data['statistics'].apply(lambda x: x[key])\n",
    "                del box_data['statistics']\n",
    "                box_data.insert(0, 'Date', date)\n",
    "                box_data.insert(1, 'Detail Path', game_id)\n",
    "                box_data.columns = box_cols\n",
    "            except:\n",
    "                box_data = ''\n",
    "\n",
    "            if params['include_plays'] == 'true':\n",
    "                try:\n",
    "                    plays_data = pd.DataFrame.from_dict(json.loads(data[0].text)['props']['pageProps']['playByPlay']['actions'])\n",
    "                    plays_data.insert(0, 'Date', date)\n",
    "                    plays_data.insert(1, 'Detail Path', game_id)\n",
    "                    plays_data.columns = play_cols\n",
    "                except:\n",
    "                    plays_data = []\n",
    "        except:\n",
    "            return 'incomplete'\n",
    "        print(game_id, end=' ')\n",
    "        final_data = {}\n",
    "        if type(box_data) != str: final_data['boxes'] = box_data\n",
    "        if (params['include_plays'] == 'true') and (len(plays_data) > 0): final_data['plays'] = plays_data\n",
    "        return final_data\n",
    "\n",
    "\n",
    "# main function\n",
    "def main(*args): # event, context\n",
    "    if args:\n",
    "        params = args[0]\n",
    "        if 'table' not in params.keys(): params['table'] = 'games'\n",
    "        if 'num_pages' not in params.keys(): params['num_pages'] = 'infer'\n",
    "        if 'batch_size' not in params.keys(): params['batch_size'] = '10'\n",
    "        if 'upload' not in params.keys(): params['upload_scrape'] = 'false'\n",
    "        if 'include_plays' not in params.keys(): params['include_plays'] = 'true'\n",
    "        if 'allow_repeats' not in params.keys(): params['allow_repeats'] = 'false'\n",
    "        if params['num_pages'] != 'infer': params['num_pages'] = int(params['num_pages'])\n",
    "        if len(args) > 1: context = args[1]\n",
    "    else:\n",
    "        params = {'table': 'games', 'num_pages': 3, 'batch_size': 10}\n",
    "\n",
    "    retries = 3\n",
    "    url_data = retrieve_urls(**params, df=True)\n",
    "    # print(params)\n",
    "    # return url_data\n",
    "    driver = make_driver()\n",
    "    num_urls = len(url_data)\n",
    "    completed = 0\n",
    "    print(num_urls, 'pages to process...')\n",
    "    while len(url_data) > 0:\n",
    "        url_data = url_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        for j in range(retries):\n",
    "\n",
    "            # read page source from the scrape, and only stay in the loop if the dict is valid\n",
    "            page_source = scrape(url_data['urls'][0], url_data['dates'][0], driver, **params)\n",
    "            if page_source == 'incomplete':\n",
    "                status = 'red'\n",
    "                if j < retries-1: time.sleep(6**(j+1))\n",
    "                continue\n",
    "            else:\n",
    "                status = 'green'\n",
    "                page_dict = parse(page_source, url_data['game_ids'][0], url_data['dates'][0], **params) \n",
    "\n",
    "                # return page_dict\n",
    "            # read dataframe from the parsed dict, and only stay in the loop if we find the data we need\n",
    "            if page_dict == 'incomplete':\n",
    "                status = 'red'\n",
    "                continue\n",
    "            elif 'games' in page_dict.keys():\n",
    "                break\n",
    "            elif 'boxes' not in page_dict.keys():\n",
    "                status = 'red'\n",
    "                print(url_data['urls'][0], 'skipping - not available')\n",
    "                url_data = url_data[url_data['urls'] != url_data['urls'][0]]\n",
    "                num_urls -= 1\n",
    "                break\n",
    "            elif 'plays' not in page_dict.keys():\n",
    "                if params['include_plays'] == 'true': print('retrieved boxes only')\n",
    "            break\n",
    "\n",
    "        time.sleep(3)\n",
    "        if status == 'green':\n",
    "            url_data = url_data[url_data['urls'] != url_data['urls'][0]]\n",
    "            if completed % int(params['batch_size']) == 0:\n",
    "                pages_dict = {}\n",
    "            for k in page_dict.keys():\n",
    "                if k not in pages_dict.keys():\n",
    "                    pages_dict[k] = page_dict[k]\n",
    "                else:\n",
    "                    pages_dict[k] = pd.concat([pages_dict[k], page_dict[k]], axis=0)\n",
    "            completed += 1\n",
    "            print(completed)\n",
    "            if completed % int(params['batch_size']) == 0 or completed == num_urls or os.path.isdir(os.path.join(mypath, 'send_s3')):\n",
    "                print('writing batch to s3...', end=' ')\n",
    "                if params['table'] != 'games':\n",
    "                    record_status(pages_dict[list(pages_dict.keys())[0]])\n",
    "\n",
    "                # we write the play by play data to a per-season file for capacity reasons\n",
    "                if 'plays' in pages_dict.keys():\n",
    "                    current_date = dt.strftime(dt.now(), '%Y-%m-%d')\n",
    "                    if current_date[-5:] < '08-15':\n",
    "                        season = '-'.join([str(int(current_date[:4]) - 1), current_date[:4]])\n",
    "                    else:\n",
    "                        season = '-'.join([current_date[:4], str(int(current_date[:4]) + 1)])\n",
    "                    pages_dict['plays/plays_'+season] = pages_dict['plays']\n",
    "                    pages_dict.pop('plays', None)\n",
    "\n",
    "                for k in pages_dict.keys():\n",
    "                    if k == 'games':\n",
    "                        subset = ['Date', 'Detail Path']\n",
    "                        sort = ['Date', 'Detail Path', 'Detail Data', 'AS']\n",
    "                        ascending = [True, True, False, False]\n",
    "                    elif k == 'boxes':\n",
    "                        subset = ['Date', 'Detail Path', 'Player ID', 'Slug']\n",
    "                        sort = ['Date', 'Detail Path']\n",
    "                        ascending = [True, True]\n",
    "                    elif re.search('plays', k) is not None:\n",
    "                        subset = ['Date', 'Detail Path', 'Action Number']\n",
    "                        sort = ['Date', 'Detail Path']\n",
    "                        ascending = [True, True]\n",
    "                    append_s3('data/' + k + '.zip', pages_dict[k].reset_index(drop=True), output='dataframe', dedupe_cols=subset, sort=sort, ascending=ascending)\n",
    "                    # append_local(os.path.join(mypath, k)+'.csv', pages_dict[k].reset_index(drop=True))\n",
    "                print('complete!')\n",
    "            if completed == num_urls - 10:\n",
    "            # params['batch_size'] = min(params['batch_size'], max(int((num_urls-completed)/5), 20)) # batch size decay as we fill up the quota\n",
    "    driver.close()\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc44f449-bf75-4b2e-91c2-f21160455a88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/fjrwkbvn1fdbd1pncpy9v4rw0000gn/T/ipykernel_9714/2889602497.py:46: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  return webdriver.Chrome(chromedriver_path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 pages to process...\n",
      "2023-03-11 1\n",
      "2023-03-12 2\n",
      "2023-03-13 3\n",
      "2023-03-15 4\n",
      "2023-03-14 5\n",
      "writing batch to s3... complete!\n"
     ]
    }
   ],
   "source": [
    "# args = {'table': 'games', 'dates': ['2023-02-08', '2023-02-20'], 'batch_size': 20}\n",
    "\n",
    "args = {\n",
    "    'table': 'games',\n",
    "    'num_pages': 'infer',\n",
    "    # 'dates': ['2023-03-07', '2023-03-07'],\n",
    "    'batch_size': 20,\n",
    "    'rand': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'include_plays': 'true',\n",
    "    'allow_repeats': 'false'\n",
    "}\n",
    "data = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46606bc3-ba26-40b6-8e7d-98c5fdb47659",
   "metadata": {},
   "source": [
    "#### **SELENIUM-TRIGGER-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f10c184-9215-4b75-b8ec-8d5148366e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import zipfile\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "def write_s3(file_path, myfile, bucket='hwm-nba', dedupe_cols=None, sort=None, compression='zip'):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    if type(myfile) == pd.core.frame.DataFrame:\n",
    "        if dedupe_cols is not None:\n",
    "            myfile = myfile.drop_duplicates(subset=dedupe_cols, keep='first')\n",
    "        if sort is not None:\n",
    "            myfile = myfile.sort_values(by=sort, ascending=True)\n",
    "        output_buffer = BytesIO() if compression == 'zip' else StringIO()\n",
    "        if compression == 'zip': file_path = '.'.join(file_path.split('.')[:-1])+'.zip'\n",
    "        myfile.to_csv(output_buffer, index=False, compression={'method':compression, 'archive_name':file_name})\n",
    "        myfile = output_buffer\n",
    "    s3_resource.Object(bucket, file_path).put(Body=myfile.getvalue())    \n",
    "\n",
    "def read_s3(file_path, bucket='hwm-nba', output=None, columns=None):\n",
    "    data = s3_resource.Object(bucket, file_path).get()['Body'].read()\n",
    "    if file_path[-3:] == 'zip':\n",
    "        data = zipfile.ZipFile(BytesIO(data))\n",
    "        data = data.read(data.namelist()[0])\n",
    "    data = data.decode('utf-8')\n",
    "    if output == 'dataframe':\n",
    "        kwargs = {'header': 0}\n",
    "        if columns is not None: kwargs['names'] = columns\n",
    "        data = pd.read_csv(StringIO(data), **kwargs, low_memory=False)\n",
    "    return data\n",
    "\n",
    "def file_checker(bucket_name, file_name, start_utc):\n",
    "    my_bucket_data = pd.DataFrame(s3_client.list_objects(Bucket=bucket_name)['Contents'])\n",
    "    updated_utc = min(my_bucket_data[my_bucket_data['Key'].isin([file_name])]['LastModified']).to_pydatetime()\n",
    "    diff = (updated_utc - start_utc)\n",
    "    return (diff.days*86400)+(diff.seconds)\n",
    "\n",
    "def retrieve_dates(dates, table='games', hour_offset=-100):\n",
    "    days = int(hour_offset//24)\n",
    "    hours = hour_offset%24\n",
    "    max_date = datetime.datetime.today() - datetime.timedelta(days=days, hours=hours)    \n",
    "    if type(dates) == list:\n",
    "        dates.sort()\n",
    "        d1, d2 = datetime.datetime.strptime(dates[0], '%Y-%m-%d'), datetime.datetime.strptime(dates[1], '%Y-%m-%d')\n",
    "        dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "    elif dates == 'infer':\n",
    "        games = read_s3('data/games.zip', output='dataframe')\n",
    "        if table != 'games': games = games[games['Detail Data'] != 0]\n",
    "        existing = list(np.unique(games['Date']))\n",
    "        max_played = np.max(np.unique(games[games['AS'] != '-']['Date']))\n",
    "        min_date = min(existing)\n",
    "        d1, d2 = datetime.datetime.strptime(min_date, '%Y-%m-%d'), max_date\n",
    "        dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "        dates = [d for d in dates if (d not in existing or d > max_played)]    \n",
    "    else:          \n",
    "        dates = [datetime.datetime.strftime(max_date - datetime.timedelta(days=d), '%Y-%m-%d') for d in range(dates)]\n",
    "    return dates\n",
    "\n",
    "def main(*args):\n",
    "    start_utc = datetime.datetime.now(datetime.timezone.utc)\n",
    "    lambda_payload_1 = args[0]\n",
    "    try:\n",
    "        print(lambda_payload_1['table'])\n",
    "    except:\n",
    "        lambda_payload_1 = eval(lambda_payload_1)\n",
    "    if 'dates' not in lambda_payload_1.keys():\n",
    "        lambda_payload_1['dates'] = []\n",
    "        date_list = retrieve_dates(lambda_payload_1['num_pages'])\n",
    "        if len(date_list) > 0:\n",
    "            lambda_payload_1['dates'].extend([date_list[0], date_list[-1]])\n",
    "\n",
    "    if len(lambda_payload_1['dates']) > 0:\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName='selenium-test',\n",
    "            InvocationType='Event', # RequestResponse\n",
    "            # response['StatusCode'] == 200:\n",
    "            Payload=json.dumps(lambda_payload_1)\n",
    "        )\n",
    "        file_found = False\n",
    "        while file_found is False:\n",
    "            diff = file_checker('hwm-nba', 'data/games.zip', start_utc)\n",
    "            print(diff)\n",
    "            if diff > 0:\n",
    "                file_found = True\n",
    "                time.sleep(6)\n",
    "                lambda_payload_2 = {\n",
    "                    'table': 'game_details',\n",
    "                    'dates': lambda_payload_1['dates'],\n",
    "                    'batch_size': lambda_payload_1['batch_size'],\n",
    "                    'rand': lambda_payload_1['rand'],\n",
    "                    'include_plays': lambda_payload_1['include_plays'],\n",
    "                    'allow_repeats': lambda_payload_1['allow_repeats']\n",
    "                }\n",
    "                print('Lambda Payload is:',lambda_payload_2)\n",
    "                lambda_client.invoke(\n",
    "                    FunctionName='selenium-trigger-2',\n",
    "                    InvocationType='Event',\n",
    "                    Payload=json.dumps(lambda_payload_2)\n",
    "                )\n",
    "            else:\n",
    "                time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5c976-51e6-401a-97f9-b09b176a8876",
   "metadata": {},
   "source": [
    "#### **SELENIUM-TRIGGER-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789fbb7-7e68-47df-b80d-b6df678c17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import zipfile\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "def write_s3(file_path, myfile, bucket='hwm-nba', dedupe_cols=None, sort=None, compression='zip'):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    if type(myfile) == pd.core.frame.DataFrame:\n",
    "        if dedupe_cols is not None:\n",
    "            myfile = myfile.drop_duplicates(subset=dedupe_cols, keep='first')\n",
    "        if sort is not None:\n",
    "            myfile = myfile.sort_values(by=sort, ascending=True)\n",
    "        output_buffer = BytesIO() if compression == 'zip' else StringIO()\n",
    "        if compression == 'zip': file_path = '.'.join(file_path.split('.')[:-1])+'.zip'\n",
    "        myfile.to_csv(output_buffer, index=False, compression={'method':compression, 'archive_name':file_name})\n",
    "        myfile = output_buffer\n",
    "    s3_resource.Object(bucket, file_path).put(Body=myfile.getvalue())    \n",
    "\n",
    "def read_s3(file_path, bucket='hwm-nba', output=None, columns=None):\n",
    "    data = s3_resource.Object(bucket, file_path).get()['Body'].read()\n",
    "    if file_path[-3:] == 'zip':\n",
    "        data = zipfile.ZipFile(BytesIO(data))\n",
    "        data = data.read(data.namelist()[0])\n",
    "    data = data.decode('utf-8')\n",
    "    if output == 'dataframe':\n",
    "        kwargs = {'header': 0}\n",
    "        if columns is not None: kwargs['names'] = columns\n",
    "        data = pd.read_csv(StringIO(data), **kwargs)\n",
    "    return data\n",
    "\n",
    "def file_checker(bucket_name, file_name, start_utc):\n",
    "    my_bucket_data = pd.DataFrame(s3_client.list_objects(Bucket=bucket_name)['Contents'])\n",
    "    updated_utc = min(my_bucket_data[my_bucket_data['Key'].isin([file_name])]['LastModified']).to_pydatetime()\n",
    "    diff = (updated_utc - start_utc)\n",
    "    return (diff.days*86400)+(diff.seconds)\n",
    "\n",
    "def retrieve_dates(dates, table='games', hour_offset=-100):\n",
    "    days = int(hour_offset//24)\n",
    "    hours = hour_offset%24\n",
    "    max_date = datetime.datetime.today() - datetime.timedelta(days=days, hours=hours)    \n",
    "    if type(dates) == list:\n",
    "        dates.sort()\n",
    "        d1, d2 = datetime.datetime.strptime(dates[0], '%Y-%m-%d'), datetime.datetime.strptime(dates[1], '%Y-%m-%d')\n",
    "        dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "    elif dates == 'infer':\n",
    "        games = read_s3('data/games.zip', output='dataframe')\n",
    "        if table != 'games': games = games[games['Detail Data'] != 0]\n",
    "        existing = list(np.unique(games['Date']))\n",
    "        max_played = np.max(np.unique(games[games['AS'] != '-']['Date']))\n",
    "        min_date = min(existing)\n",
    "        d1, d2 = datetime.datetime.strptime(min_date, '%Y-%m-%d'), max_date\n",
    "        dates = [datetime.datetime.strftime(d1+datetime.timedelta(days=d), '%Y-%m-%d') for d in range((d2-d1).days+1)]\n",
    "        dates = [d for d in dates if (d not in existing or d > max_played)]    \n",
    "    else:          \n",
    "        dates = [datetime.datetime.strftime(max_date - datetime.timedelta(days=d), '%Y-%m-%d') for d in range(dates)]\n",
    "    return dates\n",
    "\n",
    "def main(*args):\n",
    "    start_utc = datetime.datetime.now(datetime.timezone.utc)\n",
    "    lambda_payload_2 = args[0]\n",
    "    try:\n",
    "        print(lambda_payload_2['table'])\n",
    "    except:\n",
    "        lambda_payload_2 = eval(lambda_payload_2)\n",
    "    if 'dates' not in lambda_payload_2.keys():\n",
    "        date_list = retrieve_dates(lambda_payload_2['num_pages'])\n",
    "        lambda_payload_2['dates'] = [date_list[i] for i in [0,-1]]\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName='selenium-test',\n",
    "        InvocationType='Event',\n",
    "        Payload=json.dumps(lambda_payload_2)\n",
    "    )\n",
    "\n",
    "    print('Date Payload is:',lambda_payload_2['dates'])\n",
    "\n",
    "    file_found = False\n",
    "    while file_found is False:\n",
    "        diff = file_checker('hwm-nba', 'data/boxes.zip', start_utc)\n",
    "        print(diff)\n",
    "        if diff > 0:\n",
    "            file_found = True\n",
    "            return\n",
    "            time.sleep(6)\n",
    "            # lambda_payload_3 = {'table':'plays', 'dates':lambda_payload_2['dates']}\n",
    "            # lambda_client.invoke(\n",
    "            #     FunctionName='selenium-trigger-3',\n",
    "            #     InvocationType='Event',\n",
    "            #     Payload=json.dumps(lambda_payload_3)\n",
    "            # )\n",
    "        else:\n",
    "            time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1d758-2763-48e4-896a-77a6f8e7d166",
   "metadata": {},
   "source": [
    "#### **SELENIUM-TRIGGER-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d5597-d475-4ebe-b1d7-997faa84e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "lambda_client = boto3.client('lambda')\n",
    "\n",
    "def write_s3(file_path, myfile, bucket='hwm-nba', dedupe_cols=None, sort=None, compression='zip'):\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    if type(myfile) == pd.core.frame.DataFrame:\n",
    "        if dedupe_cols is not None:\n",
    "            myfile = myfile.drop_duplicates(subset=dedupe_cols, keep='first')\n",
    "        if sort is not None:\n",
    "            myfile = myfile.sort_values(by=sort, ascending=True)\n",
    "        output_buffer = BytesIO() if compression == 'zip' else StringIO()\n",
    "        if compression == 'zip': file_path = '.'.join(file_path.split('.')[:-1])+'.zip'\n",
    "        myfile.to_csv(output_buffer, index=False, compression={'method':compression, 'archive_name':file_name})\n",
    "        myfile = output_buffer\n",
    "    s3_resource.Object(bucket, file_path).put(Body=myfile.getvalue())    \n",
    "\n",
    "def main(*args):\n",
    "    lambda_payload_3 = args[0]\n",
    "    try:\n",
    "        print(lambda_payload_3['table'])\n",
    "    except:\n",
    "        lambda_payload_3 = eval(lambda_payload_3)\n",
    "\n",
    "    response = lambda_client.invoke(\n",
    "         FunctionName='selenium-test',\n",
    "         InvocationType='Event',\n",
    "         Payload=json.dumps(lambda_payload_3)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
