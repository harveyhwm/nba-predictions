{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8390f5-92c7-4ed7-982a-6b94149af6fc",
   "metadata": {},
   "source": [
    "### SCRAPE NBA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111186aa-3cc4-4525-9cba-18c05b2bf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,csv,itertools\n",
    "\n",
    "from contextlib import closing\n",
    "from copy import deepcopy\n",
    "from typing import Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re\n",
    "import lxml\n",
    "import soupsieve\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extensions\n",
    "import psycopg2.extras\n",
    "from psycopg2.extensions import connection\n",
    "from psycopg2.extras import DictCursor, NamedTupleCursor, RealDictCursor\n",
    "\n",
    "from airflow.hooks.dbapi import DbApiHook\n",
    "from airflow.models.connection import Connection\n",
    "\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "CHROMEDRIVER_PATH = '/Users/harveymanhood/chromedriver'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "#options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "#options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "service = ChromeService(executable_path=CHROMEDRIVER_PATH)\n",
    "driver = webdriver.Chrome(service=service,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a147928a-3071-424f-9cbc-5f49f04ba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/harveymanhood/Documents/- projects/- nba/data/'\n",
    "SITE_ROOT = 'https://www.nba.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290aab8-82b0-4c0b-8733-e6e76c2875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pd.read_csv('data/boxes.csv')\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ed9076-4d84-4116-a08a-747ba57be733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mitosheet\n",
    "#mitosheet.sheet(boxes, view_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "bb4742f7-1eef-47f1-9c19-dae29875c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mitosheet\n",
    "#mitosheet.sheet(boxes, view_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2961e-59b4-4ca1-b32e-2d5c59e3a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "matches = matches[(matches['AS']>'0')].reset_index(drop=True)\n",
    "matches_in = matches[(matches['Box Data']==1)] #matches = matches[(matches['AS']>'0')]\n",
    "print(set(matches['Box Data']))\n",
    "print(str(len(matches_in))+'\\n')\n",
    "matches_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd02256b-2603-4959-92dd-ca7023e41b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mitosheet\n",
    "#mitosheet.sheet(matches_in, view_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836a70d-5568-4ef8-a6a6-b184909ac970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.operators.latest_only_operator import LatestOnlyOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "import datetime\n",
    "\n",
    "#from extras import postgres_hook_batch\n",
    "#from extras.postgres_hook_batch import PostgresHookBatching\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,csv,itertools,shutil\n",
    "\n",
    "from contextlib import closing\n",
    "from copy import deepcopy\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import re\n",
    "import lxml\n",
    "import soupsieve\n",
    "\n",
    "import psycopg2.extensions\n",
    "import psycopg2.extras\n",
    "\n",
    "from airflow.hooks.dbapi import DbApiHook\n",
    "from airflow.models.connection import Connection\n",
    "\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "pg_hook = PostgresHook(postgres_conn_id='postgres_alt')\n",
    "#pg_hook_batch = PostgresHookBatching(postgres_conn_id='postgres_alt')\n",
    "\n",
    "CHROMEDRIVER_PATH = '/Users/harveymanhood/chromedriver2'\n",
    "DATA_PATH = '/Users/harveymanhood/Documents/- projects/- portfolio/nba-predictions/data/'\n",
    "SITE_ROOT = 'https://www.nba.com'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "service = ChromeService(executable_path=CHROMEDRIVER_PATH)\n",
    "\n",
    "interval_minutes_scores = 30\n",
    "interval_minutes_boxes = 30\n",
    "interval_minutes_plays = 30\n",
    "\n",
    "with DAG('nba_scores_dag',\n",
    "    start_date=datetime.datetime.now()+datetime.timedelta(hours=5)-datetime.timedelta(minutes=interval_minutes_scores+1),\n",
    "    schedule_interval='*/'+str(interval_minutes_scores)+' * * * *' #'00 00 * * *'\n",
    ") as dag:\n",
    "    def scrape_data(**kwargs):\n",
    "        numdays,offset,backfill_mode = kwargs['numdays'],kwargs['offset'],kwargs['backfill_mode']\n",
    "        if backfill_mode is True:\n",
    "            try:\n",
    "                m = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "                mdr = datetime.datetime.today() - datetime.timedelta(days=1,hours=7)\n",
    "                dr = (mdr-datetime.datetime.strptime(min(m['Date']),'%Y-%m-%d')).days\n",
    "                dl = [datetime.datetime.strftime(mdr - datetime.timedelta(days=x),'%Y-%m-%d') for x in range(dr+1+numdays)]\n",
    "                dd = list(set(m['Date']))\n",
    "                date_list = [a for a in dl if a not in dd][:numdays]\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            date_list = [datetime.datetime.strftime(datetime.datetime.today() - datetime.timedelta(days=x),'%Y-%m-%d') for x in range(offset,offset+numdays)]\n",
    "        print(date_list)\n",
    "        matches = []\n",
    "        score_df = []\n",
    "        for d in date_list:\n",
    "            if d != date_list[0]: time.sleep(3)\n",
    "            driver = webdriver.Chrome(service=service,options=options)\n",
    "            driver.get(SITE_ROOT+'/games?date='+d)\n",
    "\n",
    "            # check chromedriver version\n",
    "            # https://chromedriver.storage.googleapis.com/index.html?path=106.0.5249.61/\n",
    "\n",
    "            if 'browserVersion' in driver.capabilities:\n",
    "                print(driver.capabilities['browserVersion'])\n",
    "            else:\n",
    "                print(driver.capabilities['version'])\n",
    "\n",
    "            page_source = driver.page_source\n",
    "            soup = BS(page_source,'lxml')\n",
    "            matches = soup.select('div[class*=\"gamecard\"]')\n",
    "            for m in matches:\n",
    "                box_score = m.select('a[data-id*=\"box-score\"]')[0]\n",
    "                if len(box_score) == 0:\n",
    "                    continue\n",
    "                game_url = box_score.get_attribute_list('href')[0]\n",
    "                game = re.split('\\\\/|\\\\-',game_url.upper())\n",
    "                game_id = game_url.split('game/')[1].split('/box-score')[0]\n",
    "                gametype = 'playoffs' if len(m.select('[class*=\"gameSeriesText\"]'))>0 else 'preseason' if m.select('[data-is-preseason]')[0].get_attribute_list('data-is-preseason')[0]=='true' else 'regular'   \n",
    "                scores = [c.get_text() for c in m.select('p[class*=\"MatchupCardScore\"]')]\n",
    "                scores = ['-','-'] if len(scores)<2 else scores\n",
    "                overtime = 'Y' if m.select('p[class*=\"GameCardMatchupStatusText\"]')[0].get_text()=='FINAL/OT' else 'N'\n",
    "                data = [d,game[2],scores[0],game[4],scores[-1],overtime,game_id,gametype,0,0]\n",
    "                score_df.append(data)\n",
    "            if (matches == []) or (score_df == []):\n",
    "                score_df.append([d,'No Games',0,'No Games',0,'-','-','-',0,0])\n",
    "            driver.close()\n",
    "            driver.quit()\n",
    "        with open(DATA_PATH+'matches.pickle','wb') as token:\n",
    "            pickle.dump(score_df,token)\n",
    "\n",
    "    def transform_data(**kwargs):\n",
    "        csv_path = DATA_PATH+'matches.csv'\n",
    "        dims = ['Date','Away','AS','Home','HS','OT','Detail Path','Game Type','Box Data','Play Data']\n",
    "\n",
    "        with open(DATA_PATH+'matches.pickle','rb') as token:\n",
    "            matches = pickle.load(token)\n",
    "        matches = pd.DataFrame(matches,columns=dims)\n",
    "\n",
    "        #send to csv\n",
    "        if os.path.isfile(csv_path):\n",
    "            matches_existing = pd.read_csv(csv_path)\n",
    "            matches = (matches_existing.append(matches)).sort_values(by=dims[:3],ascending=[True,True,False]).drop_duplicates(subset=dims[:2])  #groupby(dims).sum().reset_index()\n",
    "        matches.to_csv(csv_path,index=False)\n",
    "\n",
    "        #send to postgres\n",
    "    \n",
    "    def backup_data(**kwargs):\n",
    "        csv_path = DATA_PATH+'matches.csv'\n",
    "        csv_path_new = DATA_PATH+'matches_backup.csv'\n",
    "        if os.path.exists(csv_path):\n",
    "            shutil.copy(csv_path,csv_path_new)\n",
    "\n",
    "    latest_only = LatestOnlyOperator(task_id='latest_only')\n",
    "\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=scrape_data,\n",
    "        op_kwargs={'numdays':5,'offset':1,'backfill_mode':True}\n",
    "    )\n",
    "\n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=transform_data\n",
    "    )\n",
    "\n",
    "    backup = PythonOperator(\n",
    "        task_id='backup',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=backup_data\n",
    "    )\n",
    "\n",
    "    latest_only >> backup\n",
    "    latest_only >> extract >> transform\n",
    "\n",
    "with DAG('nba_boxes_dag',\n",
    "    start_date=datetime.datetime.now()+datetime.timedelta(hours=5)-datetime.timedelta(minutes=interval_minutes_boxes+1),\n",
    "    schedule_interval='*/'+str(interval_minutes_boxes)+' * * * *' #'00 00 * * *'\n",
    ") as dag2:\n",
    "    def scrape_data_box(**kwargs):\n",
    "        matches = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "        matches = matches[(matches['Box Data']==0) & (matches['Detail Path']!='-') & (matches['AS']!='-') & (matches['Game Type']!='preseason')].reset_index(drop=True)[-5:] #sample(frac=1)\n",
    "        tdata = []\n",
    "        for m in range(len(matches)):\n",
    "            url = SITE_ROOT+'/game/'+matches.iloc[m,:]['Detail Path']+'/box-score#box-score'\n",
    "            driver = webdriver.Chrome(service=service,options=options)\n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            soup = BS(page_source,'lxml')\n",
    "            boxes = soup.select('table[class*=\"StatsTable_table\"]')\n",
    "            for b in boxes:\n",
    "                tdata.append([['PLAYER','PLAYERINIT','POSITION']])\n",
    "                for th in [e.get_text() for e in b.select('thead>tr>th')[1:]]:\n",
    "                    tdata[-1][0].append(th)\n",
    "                tdata[-1][0].append('Detail Path')\n",
    "                tr = b.select('tbody>tr')\n",
    "                for d in range(len(tr)):\n",
    "                    if d < len(tr)-1:\n",
    "                        td = tr[d].select('td')\n",
    "                        tdata[-1].append([e.get_text() for e in td[0].select('div>a>span:nth-child(2)>span,div>span:nth-child(2)')])\n",
    "                    else:\n",
    "                        tdata[-1].append(['TOTALS','',''])\n",
    "                    if len(tdata[-1][-1]) < 3:\n",
    "                        for i in range(3-len(tdata[-1][-1])): tdata[-1][-1].append('')\n",
    "                    for t in [e.get_text() for e in td[1:]]:\n",
    "                        tdata[-1][-1].append(t)\n",
    "                    if len(tdata[-1][-1]) < len(tdata[-1][0])-1:\n",
    "                        for i in range(len(tdata[-1][0])-len(tdata[-1][-1])-1): tdata[-1][-1].append('')\n",
    "                    tdata[-1][-1].append(list(matches['Detail Path'])[m])\n",
    "            driver.close()\n",
    "            driver.quit()\n",
    "            time.sleep(0.5)\n",
    "        with open(DATA_PATH+'boxes.pickle','wb') as token:\n",
    "            pickle.dump(tdata,token)\n",
    "\n",
    "    def transform_data_box(**kwargs):\n",
    "        csv_path = DATA_PATH+'boxes.csv'\n",
    "        if os.path.isfile(csv_path):\n",
    "            boxes_df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            boxes_df = None\n",
    "        with open(DATA_PATH+'boxes.pickle','rb') as token:\n",
    "            boxes = pickle.load(token)\n",
    "        for b in range(len(boxes)):\n",
    "            box = pd.DataFrame(boxes[b])\n",
    "            box.columns = box.iloc[0,:]\n",
    "            box['Home'] = 'Y' if b%2==1 else 'N'\n",
    "            if boxes_df is None:\n",
    "                boxes_df = box.iloc[1:,:]\n",
    "            else:\n",
    "                boxes_df = boxes_df.append(box.iloc[1:,:])\n",
    "        boxes_df = boxes_df.drop_duplicates()\n",
    "        boxes_df.to_csv(csv_path,index=False)\n",
    "        boxes_df['count'] = 1\n",
    "        boxes_df = boxes_df.loc[:,['Detail Path','count']].drop_duplicates()\n",
    "        matches = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "        matches = matches.merge(boxes_df,how='left',on=['Detail Path'],sort=False)\n",
    "        matches['Box Data'] = matches.apply(lambda x: 1 if x['count'] == 1 else (-1 if x['Box Data'] == 1 else x['Box Data']),axis=1)\n",
    "        del matches['count']\n",
    "        matches.to_csv(DATA_PATH+'matches.csv',index=False)\n",
    "\n",
    "    latest_only = LatestOnlyOperator(task_id='latest_only')\n",
    "\n",
    "    def backup_data_box(**kwargs):\n",
    "        csv_path = DATA_PATH+'boxes.csv'\n",
    "        csv_path_new = DATA_PATH+'boxes_backup.csv'\n",
    "        if os.path.exists(csv_path):\n",
    "            if os.path.exists(csv_path_new):\n",
    "                if os.path.getsize(DATA_PATH+'boxes.csv') >= os.path.getsize(DATA_PATH+'boxes_backup.csv'):\n",
    "                    shutil.copy(csv_path,csv_path_new)\n",
    "            else:\n",
    "                shutil.copy(csv_path,csv_path_new)\n",
    "            if datetime.datetime.now().minute == 0:\n",
    "                shutil.copy(csv_path,re.sub('\\.csv','_'+str(datetime.datetime.now().microsecond)+'.csv',csv_path_new))\n",
    "\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=scrape_data_box,\n",
    "        op_kwargs={'num':3}\n",
    "    )\n",
    "\n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=transform_data_box\n",
    "    )\n",
    "\n",
    "    backup = PythonOperator(\n",
    "        task_id='backup',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=backup_data_box\n",
    "    )\n",
    "\n",
    "    latest_only >> backup\n",
    "    latest_only >> extract >> transform\n",
    "\n",
    "with DAG('nba_plays_dag',\n",
    "    start_date=datetime.datetime.now()+datetime.timedelta(hours=5)-datetime.timedelta(minutes=interval_minutes_plays+1),\n",
    "    schedule_interval='*/'+str(interval_minutes_plays)+' * * * *' #'00 00 * * *'\n",
    ") as dag3:\n",
    "    def scrape_data_plays(**kwargs):\n",
    "        matches = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "        matches = matches[(matches['Play Data']==0) & (matches['Detail Path']!='-') & (matches['AS']!='-')].reset_index(drop=True)[-5:] #sample(frac=1) #& (matches['Game Type']!='preseason')\n",
    "        tdata = [['Minute','Score','Action','Team','Detail Path']]\n",
    "        for m in range(len(matches)):\n",
    "            detail_path = matches.iloc[m,:]['Detail Path']\n",
    "            url = SITE_ROOT+'/game/'+detail_path+'/play-by-play?period=All'\n",
    "            driver = webdriver.Chrome(service=service,options=options)\n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            soup = BS(page_source,'lxml') #,'html.parser')\n",
    "            plays = soup.select('article[class*=\"GamePlayByPlayRow_article\"]')\n",
    "            for p in plays:\n",
    "                clock = p.select('span[class*=\"GamePlayByPlayRow_clock\"]')[0].get_text()\n",
    "                score = p.select('span[class*=\"GamePlayByPlayRow_scoring\"]')\n",
    "                score = score[0].get_text() if len(score)>0 else ''\n",
    "                desc = p.select('span[class*=\"GamePlayByPlayRow_desc\"]')[0].get_text()\n",
    "                team = 'Home' if p.get_attribute_list('data-is-home-team')[0]=='true' else 'Away'\n",
    "                tdata.append([clock,score,desc,team,detail_path])\n",
    "            driver.close()\n",
    "            driver.quit()\n",
    "            time.sleep(0.5)\n",
    "        with open(DATA_PATH+'plays.pickle','wb') as token:\n",
    "            pickle.dump(tdata,token)\n",
    "\n",
    "    def transform_data_plays(**kwargs):\n",
    "        csv_path = DATA_PATH+'plays.csv'\n",
    "        if os.path.isfile(csv_path):\n",
    "            plays_df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            plays_df = None\n",
    "        with open(DATA_PATH+'plays.pickle','rb') as token:\n",
    "            plays = pickle.load(token)\n",
    "        plays = pd.DataFrame(plays)\n",
    "        plays.columns = plays.iloc[0,:]\n",
    "        if plays_df is None:\n",
    "            plays_df = plays.iloc[1:,:]\n",
    "        else:\n",
    "            plays_df = plays_df.append(plays.iloc[1:,:])\n",
    "        plays_df = plays_df.drop_duplicates()\n",
    "        plays_df.to_csv(csv_path,index=False)\n",
    "        plays_df['count'] = 1\n",
    "        plays_df = plays_df.loc[:,['Detail Path','count']].drop_duplicates()\n",
    "        matches = pd.read_csv(DATA_PATH+'matches.csv')\n",
    "        matches = matches.merge(plays_df,how='left',on=['Detail Path'],sort=False)\n",
    "        matches['Play Data'] = matches.apply(lambda x: 1 if x['count'] == 1 else (-1 if x['Play Data'] == 1 else x['Play Data']),axis=1)\n",
    "        del matches['count']\n",
    "        matches.to_csv(DATA_PATH+'matches.csv',index=False)\n",
    "\n",
    "    latest_only = LatestOnlyOperator(task_id='latest_only')\n",
    "\n",
    "    def backup_data_plays(**kwargs):\n",
    "        csv_path = DATA_PATH+'plays.csv'\n",
    "        csv_path_new = DATA_PATH+'plays_backup.csv'\n",
    "        if os.path.exists(csv_path):\n",
    "            if os.path.exists(csv_path_new):\n",
    "                if os.path.getsize(DATA_PATH+'plays.csv') >= os.path.getsize(DATA_PATH+'plays_backup.csv'):\n",
    "                    shutil.copy(csv_path,csv_path_new)\n",
    "            else:\n",
    "                shutil.copy(csv_path,csv_path_new)\n",
    "            if datetime.datetime.now().minute == 0:\n",
    "                shutil.copy(csv_path,re.sub('\\.csv','_'+str(datetime.datetime.now().microsecond)+'.csv',csv_path_new))\n",
    "\n",
    "    extract = PythonOperator(\n",
    "        task_id='extract',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=scrape_data_plays,\n",
    "        op_kwargs={'num':3}\n",
    "    )\n",
    "\n",
    "    transform = PythonOperator(\n",
    "        task_id='transform',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=transform_data_plays\n",
    "    )\n",
    "\n",
    "    backup = PythonOperator(\n",
    "        task_id='backup',\n",
    "        trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=backup_data_plays\n",
    "    )\n",
    "\n",
    "    latest_only >> backup\n",
    "    latest_only >> extract >> transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 tf1",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
